{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semesteroppgave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation of test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data preparation \n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "# imports for machine learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Auxiliary\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "class TaggedPoint():\n",
    "    def __init__(self, data, tag):\n",
    "        self.data = data\n",
    "        self.tag = tag\n",
    "\n",
    "# sentiment analyse for nowegain, norec corpus\n",
    "# norec sentences , github\n",
    "# 123 neg, 56 pos\n",
    "# aruca finetuning sentiment\n",
    "# small norbert3 \n",
    "# kushtrin, sentiment \n",
    "# mlp classifier på topp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investageting the size of the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(pn):\n",
    "    for folder in os.listdir(pn):\n",
    "        new_path = pn+folder\n",
    "        if os.path.isdir(new_path):\n",
    "            folder_size = sum(os.path.getsize(os.path.join(new_path, file)) for file in os.listdir(new_path))\n",
    "            folder_size_kb = round(folder_size / 1024, 2)      \n",
    "            num_files = sum(1 for f in os.listdir(\"./\"+new_path))\n",
    "\n",
    "        \n",
    "        print(f\"{folder} size: {folder_size_kb} kB, {num_files} files\")\n",
    "    print(\"\\n\")\n",
    "    #print(f\"{folder}_size: {os.path.getsize()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VG contains: \n",
      "2005 size: 50307.78 kB, 264 files\n",
      "2006 size: 74914.96 kB, 360 files\n",
      "2007 size: 67835.89 kB, 351 files\n",
      "2008 size: 50588.86 kB, 359 files\n",
      "2009 size: 148471.12 kB, 352 files\n",
      "2010 size: 129299.64 kB, 359 files\n",
      "2011 size: 103358.62 kB, 359 files\n",
      "\n",
      "\n",
      "Fedrelandsvennen contains : \n",
      "2005 size: 34287.34 kB, 260 files\n",
      "2006 size: 53280.71 kB, 359 files\n",
      "2007 size: 288423.57 kB, 351 files\n",
      "2008 size: 79907.46 kB, 343 files\n",
      "2009 size: 33930.09 kB, 166 files\n",
      "2010 size: 88293.11 kB, 361 files\n",
      "2011 size: 75608.53 kB, 359 files\n",
      "\n",
      "\n",
      "Nordlys contains: \n",
      "2005 size: 50307.78 kB, 264 files\n",
      "2006 size: 74914.96 kB, 360 files\n",
      "2007 size: 67835.89 kB, 351 files\n",
      "2008 size: 50588.86 kB, 359 files\n",
      "2009 size: 148471.12 kB, 352 files\n",
      "2010 size: 129299.64 kB, 359 files\n",
      "2011 size: 103358.62 kB, 359 files\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"VG contains: \")\n",
    "check_dir(\"./vg/\")\n",
    "\n",
    "print(\"Fedrelandsvennen contains : \")\n",
    "check_dir(\"./fv/\")\n",
    "\n",
    "print(\"Nordlys contains: \")\n",
    "check_dir(\"./vg/\")\n",
    "\n",
    "\n",
    "# test set på år \n",
    "# trnne på norec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the test data and storing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text prepartion function\n",
    "\n",
    "def clean_text_func(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"¶\", \"\")\n",
    "    text = re.sub(r'<[^>]*>|^\\s*</[^>]*>$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^\\w\\s.,!?]|\\_\\w{35,}', '', text, flags=re.MULTILINE)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def create_test_dir(dir_path):\n",
    "    pattern = re.compile(r\">(.*?)##\", re.DOTALL) # regex compile pattern\n",
    "\n",
    "    for file_name in os.listdir(f\"{dir_path}\"): # iterate every filname in target dir\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        with open(file_path, \"r\", encoding=\"latin-1\") as infile: # read files\n",
    "            html_text = infile.read()\n",
    "            matches = pattern.findall(html_text) # separates every article based on regex pattern (from > to ##)\n",
    "            \n",
    "                        \n",
    "        file_wo_format = file_name[:-6] # file name without format\n",
    "        path_split = dir_path.split(\"/\") \n",
    "        new_dir_path = f\"./test_{path_split[-2]}/test_{path_split[-1]}\" \n",
    "        os.makedirs(new_dir_path,exist_ok=True)\n",
    "\n",
    "        with open(f\"./{new_dir_path}/{file_wo_format}.txt\", \"w\") as outfile: # cleans text further and writes to new format\n",
    "            for text in matches:\n",
    "                cleaned_text = clean_text_func(text.strip())\n",
    "                outfile.write(cleaned_text.strip()+\"\\n\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating test file directories for different news papers with years\n",
    "\n",
    "search_paths = [\"./vg\", \"./fv\", \"./nl\"] \n",
    "\n",
    "\n",
    "for paths in search_paths:\n",
    "    for path in os.listdir(paths):\n",
    "        create_test_dir(f\"./vg/{path}\")\n",
    "        create_test_dir(f\"./{paths}/{path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting articles that does not meet the requirements (elimination by search phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used for iterating through each new test folder\n",
    "# takes list as param to find kewords and rearrange articles\n",
    "\n",
    "def phrase_search(path, phrases):\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(phrases) + r')\\b', re.IGNORECASE)\n",
    "    exclude_words = {\"eksamen\", \"Eksamen\"}\n",
    "\n",
    "    for fn in os.listdir(path):\n",
    "        fp = path+\"/\"+fn\n",
    "        sp = fp.split(\"/\")\n",
    "\n",
    "        with open(fp, \"r\", encoding=\"latin-1\") as file:\n",
    "            content = file.read()\n",
    "            articles = re.split(r'\\n\\s*\\n', content.strip())\n",
    "\n",
    "\n",
    "        filtered_articles = []\n",
    "        for article in articles:\n",
    "            if re.search(pattern, article):\n",
    "                if not any(exclude in article.lower() for exclude in exclude_words):\n",
    "                    filtered_articles.append(article)\n",
    "\n",
    "\n",
    "        with open(f\"./{sp[-3]}_sami_articles.txt\", \"a\", encoding=\"latin-1\") as file:\n",
    "            file.write(\"\\n\\n\\n\".join(filtered_articles))\n",
    "            #file.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of search paths from work work directory\n",
    "# including directory for vg, fedrelandsvennen, nordlys\n",
    "\n",
    "\n",
    "\n",
    "search_phrases = [\"same\", \"samer\", \"samene\", \"samisk\", \"sápmi\", \"sametinget\", \"reindrift\", \"urbefolkning\"]\n",
    "search_phrases = search_phrases+[word.capitalize() for word in search_phrases]\n",
    "search_tpaths = [\"./test_vg\", \"./test_fv\", \"./test_nl\"] \n",
    "\n",
    "\n",
    "\n",
    "for paths in search_tpaths:\n",
    "    for path in os.listdir(paths):\n",
    "        phrase_search(f\"./{paths}/{path}\", search_phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further text preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
